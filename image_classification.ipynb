{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # For number storage and calculations\n",
    "import os # For files\n",
    "from sklearn.metrics import confusion_matrix # To evaluate the correctness of our model\n",
    "import seaborn as sn; sn.set(font_scale=1.4) # To format our charts\n",
    "from sklearn.utils import shuffle # To shuffle input data\n",
    "import matplotlib.pyplot as plt # To plot our progress\n",
    "import cv2 # Computer vision framework\n",
    "import tensorflow as tf # Neural network framework\n",
    "from tensorflow import keras # To build the layers of a network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recursively import the data from a downloaded folder\n",
    "class_names = ['benign', 'malignant']\n",
    "class_names_label = {class_name: i for i, class_name in enumerate(class_names)}\n",
    "nb_classes = len(class_names)\n",
    "IMAGE_SIZE = (150, 150)\n",
    "def load_data():\n",
    "    DIRECTORY = r\"C:\\Users\\faith\\OneDrive\\Documents\\Image_Classification\"\n",
    "    CATEGORY = [\"train\", \"test\"]\n",
    "    output = []\n",
    "    for category in CATEGORY:\n",
    "        path = os.path.join(DIRECTORY, category)\n",
    "        print(path)\n",
    "        images = []\n",
    "        labels = []\n",
    "        print(\"Loading {}\".format(category))\n",
    "        for folder in os.listdir(path):\n",
    "            label = class_names_label[folder]\n",
    "            for file in os.listdir(os.path.join(path, folder)):\n",
    "                img_path = os.path.join(os.path.join(path, folder), file)\n",
    "                image = cv2.imread(img_path)\n",
    "\n",
    "                # Convert default color formatting for CV2\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "                # Resize an image to a standard size \n",
    "                image = cv2.resize(image, IMAGE_SIZE)\n",
    "\n",
    "                # Add the images and labels to corresponding lists\n",
    "                images.append(image)\n",
    "                labels.append(label)\n",
    "\n",
    "        # Convert the lists into numpy arrays \n",
    "        images = np.array(images, dtype = 'float32')\n",
    "        labels = np.array(labels, dtype = 'int32')\n",
    "        output.append((images, labels))\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate a portion of the data to use later to evaluate the model\n",
    "(train_images, train_labels), (test_images, test_labels) = load_data()\n",
    "\n",
    "# Shuffle the image order to gain a more general understanding of the data during training\n",
    "train_images, train_labels = shuffle(train_images, train_labels, random_state=25)\n",
    "test_images, test_labels = shuffle(test_images, test_labels, random_state=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count to ensure malignant and benign samples are close in size for training\n",
    "zeros = 0\n",
    "ones = 1\n",
    "for i in range(len(train_labels)):\n",
    "    # print(train_labels[i])\n",
    "    if(train_labels[i]==0):\n",
    "        zeros+=1\n",
    "    elif(train_labels[i]==1):\n",
    "        ones+=1\n",
    "print(\"zeros: \" + str(zeros))\n",
    "print(\"ones: \" + str(ones))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same as above for testing\n",
    "zeros = 0\n",
    "ones = 1\n",
    "for i in range(len(test_labels)):\n",
    "    # print(train_labels[i])\n",
    "    if(test_labels[i]==0):\n",
    "        zeros+=1\n",
    "    elif(test_labels[i]==1):\n",
    "        ones+=1\n",
    "print(\"zeros: \" + str(zeros))\n",
    "print(\"ones: \" + str(ones))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic model\n",
    "the_model = tf.keras.Sequential([ \n",
    "    tf.keras.layers.Conv2D(32, (3,3), activation = 'relu', input_shape = (150, 150, 3)), # Input layer has the dimensions of our image\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "    keras.layers.Dropout(0.5), # Reduces overfitting\n",
    "    tf.keras.layers.Dense(2, activation=tf.nn.softmax), # Two dimensional because of the two outputs\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced model\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\n",
    "\n",
    "# Increased Convolutional layers\n",
    "model = Sequential() \n",
    "model.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', activation ='relu', input_shape = (150,150,3)))\n",
    "model.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', activation ='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', activation ='relu'))\n",
    "model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', activation ='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation = \"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(2, activation = \"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resnet\n",
    "\n",
    "from tensorflow.python.keras.layers import Dense, Flatten, BatchNormalization,Dropout,Conv2D,MaxPool2D\n",
    "from keras.applications.resnet import ResNet50\n",
    "resnet_weights_path = '../Image_Classification/resnet50_weights_tf_dim_ordering_tf_kernels.h5' \n",
    "\n",
    "# Residual networks are a gateless deep feedforward network\n",
    "model = Sequential()\n",
    "model.add(ResNet50(include_top=False,input_tensor=None,input_shape=(224,224,3),pooling='avg',classes=2,weights=resnet_weights_path))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.layers[0].trainable = False\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy and validation accuracy of the model training\n",
    "def plot_accuracy(history):\n",
    "    fig=plt.figure(figsize=(10,5))\n",
    "    plt.subplot(221)\n",
    "    plt.plot(history.history['accuracy'], 'bo--', label='acc')\n",
    "    plt.plot(history.history['val_accuracy'], 'ro--', label=\"val_acc\")\n",
    "    plt.title(\"train_acc vs val_acc\")\n",
    "    plt.ylabel(\"accuracy\")\n",
    "    plt.xlabel(\"epochs\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorboard is used to chart the progress and structure of our model\n",
    "# It runs based on the logs specified in a directory, which must be created at each run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "log_dir = os.path.join('logs','fit', datetime.datetime.now().strftime('%Y%m%d-%H%M%S'))\n",
    "print(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!powershell rm -Force -R logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(log_dir, exist_ok=True)\n",
    "!powershell dir logs\\fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the randomization of the test labels\n",
    "test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model using an optimizer that can handle sparse gradients on noisy problems. Displays the accuracy after each epoch\n",
    "the_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allow tensorboard to access the model's data\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir = log_dir, histogram_freq = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model with the images and labels\n",
    "# Batch size was experimented with and 128 was the optimal number for the size of the dataset\n",
    "# The optimal number of epochs depends on the model but higher is better up until a plateau\n",
    "# Twenty percent of the data is saved for the model to validate and learn\n",
    "# The validation data is shuffled\n",
    "history = the_model.fit(train_images, train_labels, batch_size=128, epochs=5, validation_split=0.2, callbacks=[tensorboard_callback], shuffle=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accuracy(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unnecessary weights in the model\n",
    "\n",
    "import tensorflow_model_optimization as tfmot\n",
    "pruning_schedule = tfmot.sparsity.keras.ConstantSparsity(\n",
    "          target_sparsity=0.8,\n",
    "          begin_step=0,\n",
    "          end_step=1000,\n",
    ")\n",
    "\n",
    "\n",
    "pruned_model = tfmot.sparsity.keras.prune_low_magnitude(\n",
    "    model, pruning_schedule=pruning_schedule\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a confusion matrix for the model:\n",
    "# True positive, False negative\n",
    "# False negative, True negative\n",
    "\n",
    "preds = model.predict(test_images)\n",
    "rev = []\n",
    "for row in preds:\n",
    "    if row[1] == 1:\n",
    "        rev.append(1)\n",
    "    else:\n",
    "        rev.append(0)\n",
    "rev = np.array(rev)\n",
    "cm = confusion_matrix(test_labels, rev)\n",
    "print(cm)\n",
    "sn.heatmap(cm, cmap='Blues')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean tensorboard after the session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! powershell \"echo 'checking for existing tensorboard processes'\"\n",
    "! powershell \"ps | Where-Object {$_.ProcessName -eq 'tensorboard'}\"\n",
    "\n",
    "! powershell \"ps | Where-Object {$_.ProcessName -eq 'tensorboard'}| %{kill $_}\"\n",
    "\n",
    "! powershell \"echo 'cleaning tensorboard temp dir'\"\n",
    "! powershell \"rm $env:TEMP\\.tensorboard-info\\*\"\n",
    "\n",
    "! powershell \"ps | Where-Object {$_.ProcessName -eq 'tensorboard'}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir=\"logs/fit\" --host localhost \n",
    "\n",
    "\n",
    "! echo If it has timed out in jupyter, then go to http://localhost:6006 in the browser and check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model as an .hdf5 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"Image_Classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model = input(\"Do you wish to save this model [y/n]: \").strip().lower()\n",
    "\n",
    "if save_model == 'y' or save_model == 'yes':\n",
    "    model_name = input(\"Model Name: \").strip()\n",
    "    try:\n",
    "        tf.keras.models.save_model(model, model_name)\n",
    "    except:\n",
    "        print(\"Saving failed...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('imgclass')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4e2fc15d03b15039dccae38aac24bfc1c312ceffc98340a61804b1935939d45d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
